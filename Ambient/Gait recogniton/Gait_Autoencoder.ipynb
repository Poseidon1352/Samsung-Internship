{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#libraries\n",
    "%matplotlib notebook\n",
    "\n",
    "import wavio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from python_speech_features import mfcc\n",
    "from IPython.core.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Preprocessing\n",
    "* d0.wav and d1.wav contain sound produced by the footsteps of two distinct people.\n",
    "* Preprocessed using audacity to remove silence.\n",
    "* Imported and converted to MFCC vectors using suitable fixed window. Tune window parameters for better performance.\n",
    "* Concatenated into one training vector.\n",
    "* Standardized to $ \\mu = 0, \\sigma = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x0 = wavio.read('at7/d0.wav').data; x1 = wavio.read('at7/d1.wav').data\n",
    "l0 = mfcc(x0[:,0],44100,0.2,0.1); l1 = mfcc(x1[:,0],44100,0.2,0.1)\n",
    "fx = np.concatenate((l0,l1),0)\n",
    "train_x = np.array(fx); mu = np.mean(train_x); sigma = np.std(train_x); #data standardization\n",
    "train_x = (train_x - mu + 0.0)/sigma\n",
    "\n",
    "num_classes = 2;\n",
    "n_train = fx.shape[0]\n",
    "n0 = l0.shape[0]\n",
    "input_size = l0.shape[1]\n",
    "x = tf.placeholder(tf.float32, [None, input_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "* Adjust hidden_shape to add layers or neurons within the layers. Eg: [100,2,100] <=> 3 hidden layers with 100,2,100 neurons in the 1st,2nd,3rd layer respectively.\n",
    "* Edit act_fns, code_lyr according to hidden_shape. Here the sparse representation is hidden layer no 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_shape = [100,2,100] #No of neurons in each hidden layer\n",
    "act_fns = ['tanh','sig','tanh'] #Activation functions of layers\n",
    "code_lyr = 2 #index of hidden layer containing sparse vector\n",
    "num_lyr = len(hidden_shape) #No of hidden layers\n",
    "net_shape = [input_size] + hidden_shape + [input_size]\n",
    "learning_rate = tf.placeholder(tf.float32,[])\n",
    "beta = tf.placeholder(tf.float32,[]) #sparsity_penalty weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "params = {'W':{},'b':{}} #W: Weights, b: biases\n",
    "for lay_num in range(num_lyr+1):\n",
    "    params['W'][lay_num] = tf.Variable(tf.random_normal([net_shape[lay_num], net_shape[lay_num+1]],stddev = 0.5))\n",
    "    params['b'][lay_num] = tf.Variable(tf.random_normal([net_shape[lay_num+1]],stddev = 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Graph\n",
    "* Cost = Recontruction error + beta*Sparsity penalty\n",
    "* Recontruction error is the L2 norm between the input and output (reconstructed) vector. This is usual for continuous input autoencoders\n",
    "* The sparsity penalty is nonstandard. It is applied on the \"sparse\" 2-vector. Since it is output from a sigmoid layer, it lies in unit square centered at (0.5,0.5). A decreasing function of the distance from the line y=x is used. This ensures minima that are furthest apart in 2D space [At (1,0) and (0,1)]. Modify this function according to the output space of the 2-vector layer.\n",
    "* The intuition behind the setup is that if the input contains MFCCs from two distinct people (i.e feature values from distinct classes), they should settle at the two distinct minima. This is because settling farther away from each other allows greater freedom in representational power for reconstruction. \n",
    "* This also allows use to deterministically say where the clusters will converge. One class should converge above the line y = x and another, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#network graph\n",
    "def act_fn(a,lay_num):\n",
    "    if act_fns[lay_num] == 'tanh':\n",
    "        return tf.tanh(a)\n",
    "    elif act_fns[lay_num] == 'relu':\n",
    "        return tf.nn.relu(a)\n",
    "    elif act_fns[lay_num] == 'elu':\n",
    "        return tf.nn.elu(a)\n",
    "    elif act_fns[lay_num] == 'sig':\n",
    "        return tf.sigmoid(a)\n",
    "    \n",
    "def AutoEncoder(x,params):\n",
    "    z = x\n",
    "    for lay_num in range(num_lyr):\n",
    "        z = act_fn(tf.add(tf.matmul(z,params['W'][lay_num]),params['b'][lay_num]),lay_num)\n",
    "        if lay_num == code_lyr-1:\n",
    "            corner_pusher = -tf.reduce_mean(tf.abs(tf.sub(z[:,0],z[:,1])))/np.sqrt(2) #distance from line y = x\n",
    "            sparsity_penalty = corner_pusher\n",
    "            code = z\n",
    "    x_ = tf.add(tf.matmul(z,params['W'][lay_num+1]),params['b'][lay_num+1])\n",
    "    cost = tf.sqrt(tf.reduce_mean(tf.squared_difference(x,x_))) + tf.mul(beta,sparsity_penalty)\n",
    "    return (cost,code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Construction\n",
    "* Ordinary gradient descent optimization is used. Other optimizers may be considered to automate the learning process better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost,code = AutoEncoder(x,params)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameter initialization\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning\n",
    "* The model should have sufficient representational power:\n",
    "    * The size of the sparse vector should scale with the number of clusters (and hence classes) we want to detect. Here num_classes = 2. Size(sparse vector) = 2 proved to be sufficient.\n",
    "    * Set beta = 0 and adjust the size of other layers until the reconstruction error is low.\n",
    "* Beta should be small enough that it doesn't substantitally affect the recontructive capability but still pushes the points to the desired peripheries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#learning\n",
    "n_iter = 0\n",
    "while True:\n",
    "    try:\n",
    "        _,cost_,code_ = sess.run([optimizer,cost,code],feed_dict={x:train_x,learning_rate:0.1,beta:0.05})\n",
    "        if n_iter % 1000 == 0:\n",
    "            print('iter'+str(n_iter)+' cost:'+str(cost_))\n",
    "        if n_iter+1 % 20000 == 0:\n",
    "            clear_output()\n",
    "        n_iter += 1\n",
    "    except KeyboardInterrupt:\n",
    "        print \"Training is stopped\"\n",
    "        break\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plots\n",
    "plt.title('Sparse Vectors'); plt.xlabel('Dimension 1'); plt.ylabel('Dimension 2');\n",
    "plt.scatter(code_[n0:][:,0],code_[n0:][:,1],c=[1,0,0], label = 'Person 1')\n",
    "plt.scatter(code_[:n0][:,0],code_[:n0][:,1],c=[0,0,1], label = 'Person 2')\n",
    "plt.legend()\n",
    "plt.plot([0,1],[0,1],c=[0,0,0],linewidth=2.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction Confidence and Classification\n",
    "* Each sparse vector (code_[i] where i is the index) is classified as the red class if y > x and blue class otherwise.\n",
    "* Prediction confidence (after seeing n samples)  = $\\dfrac{No\\  of\\  correctly\\  classified\\  points\\ upto\\ now}{n}$. If this number is over 0.5 i.e the clustering algorithm thinks it is more likely that the points belong to the correct class, we make the right prediction.\n",
    "* If we want higher confidence estimates, especially when there is sizeable overlap betwwen the two cluster near the boundary (line y = x), We may want to throw away points close to the boundary. They don't help us discriminate much as they may be generated by noise or are low confidence signal points. An alternate definition of prediction confidence is then required: $ Prediction\\ confidence[n]  =\\dfrac{No\\  of\\  correctly\\  classified\\  points - No\\  of\\  misclassified\\  points}{n}$. In this case, we require that prediction confidence > 0 (as opposed to 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "np.cumsum((code_[n0:,0]< code_[n0:,1])+0.0)/(range(1,l1.shape[0]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.cumsum((code_[:n0,0] > code_[:n0,1])+0.0)/(range(1,l0.shape[0]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
