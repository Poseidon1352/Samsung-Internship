{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "#%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.core.display import clear_output\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\")\n",
    "train_x = np.concatenate((mnist.train.images[mnist.train.labels == 0],mnist.train.images[mnist.train.labels == 1],mnist.train.images[mnist.train.labels == 2],mnist.train.images[mnist.train.labels == 6]))\n",
    "# train_x = np.array(train_x); mu = np.mean(train_x); sigma = np.std(train_x); #data standardization\n",
    "# train_x = (train_x - mu + 0.0)/sigma\n",
    "\n",
    "n_train = train_x.shape[0]\n",
    "n0 = np.sum(mnist.train.labels == 0); n1 = np.sum(mnist.train.labels == 1); n2 = np.sum(mnist.train.labels == 2); n3 = np.sum(mnist.train.labels == 5)\n",
    "e1 = n0+n1; e2 = n0+n1+n2;\n",
    "input_size = train_x.shape[1]\n",
    "x = tf.placeholder(tf.float32, [None, input_size])\n",
    "momentum = tf.placeholder(tf.float32,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_shape = [150,2,150] #No of neurons in each hidden layer\n",
    "act_fns = ['tanh','tanh','tanh'] #Activation functions of layers\n",
    "code_lyr = 2 #index of hidden layer containing sparse vector\n",
    "num_lyr = len(hidden_shape) #No of hidden layers\n",
    "net_shape = [input_size] + hidden_shape + [input_size]\n",
    "learning_rate = tf.placeholder(tf.float32,[])\n",
    "beta = tf.placeholder(tf.float32,[]) #sparsity_penalty weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "params = {'W':{},'b':{}} #W: Weights, b: biases\n",
    "for lay_num in range(num_lyr+1):\n",
    "    params['W'][lay_num] = tf.Variable(tf.random_normal([net_shape[lay_num], net_shape[lay_num+1]],stddev = 0.05),name='weights_layer'+str(lay_num))\n",
    "    params['b'][lay_num] = tf.Variable(tf.random_normal([net_shape[lay_num+1]],stddev = 0.05),name='biases_layer'+str(lay_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#network graph\n",
    "def act_fn(a,lay_num):\n",
    "    if act_fns[lay_num] == 'tanh':\n",
    "        return tf.tanh(a)\n",
    "    elif act_fns[lay_num] == 'relu':\n",
    "        return tf.nn.relu(a)\n",
    "    elif act_fns[lay_num] == 'elu':\n",
    "        return tf.nn.elu(a)\n",
    "    elif act_fns[lay_num] == 'sig':\n",
    "        return tf.sigmoid(a)\n",
    "    \n",
    "def AutoEncoder(x,params):\n",
    "    z = x\n",
    "    for lay_num in range(num_lyr):\n",
    "        z = act_fn(tf.add(tf.matmul(z,params['W'][lay_num]),params['b'][lay_num]),lay_num)\n",
    "        if lay_num == code_lyr-1:\n",
    "            wall_pusher = -tf.reduce_mean(tf.sqrt(tf.add(tf.square(z[:,0]),tf.square(z[:,1])))) #distance from (0,0)\n",
    "            sparsity_penalty = wall_pusher\n",
    "            code = z\n",
    "    x_ = tf.add(tf.matmul(z,params['W'][lay_num+1]),params['b'][lay_num+1])\n",
    "    cost = tf.sqrt(tf.reduce_mean(tf.squared_difference(x,x_))) + tf.mul(beta,sparsity_penalty)\n",
    "    return (cost,code,x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost,code,x_ = AutoEncoder(x,params)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#parameter initialization\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter0 cost:0.144642\n",
      "iter10 cost:0.124795\n",
      "iter20 cost:0.118225\n",
      "iter30 cost:0.113718\n",
      "iter40 cost:0.110312\n",
      "iter50 cost:0.107681\n",
      "iter60 cost:0.105626\n",
      "iter70 cost:0.104005\n",
      "iter80 cost:0.102711\n",
      "iter90 cost:0.101662\n",
      "iter100 cost:0.100797\n",
      "iter110 cost:0.100073\n",
      "iter120 cost:0.0994591\n",
      "iter130 cost:0.09893\n",
      "iter140 cost:0.0984682\n",
      "iter150 cost:0.0980604\n",
      "iter160 cost:0.0976982\n",
      "iter170 cost:0.0973742\n",
      "iter180 cost:0.0970825\n",
      "iter190 cost:0.0968184\n",
      "Training is stopped\n"
     ]
    }
   ],
   "source": [
    "#learning\n",
    "n_iter = 0\n",
    "while True:\n",
    "    try:\n",
    "        _,cost_,code_,x__ = sess.run([optimizer,cost,code,x_],feed_dict={x:train_x,learning_rate:0.5,beta:0.1})\n",
    "        if n_iter % 10 == 0:\n",
    "            print('iter'+str(n_iter)+' cost:'+str(cost_))\n",
    "        if n_iter+1 % 20 == 0:\n",
    "            clear_output()\n",
    "        n_iter += 1\n",
    "    except KeyboardInterrupt:\n",
    "        print \"Training is stopped\"\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plots\n",
    "plt.title('Sparse Vectors'); plt.xlabel('Dimension 1'); plt.ylabel('Dimension 2');\n",
    "plt.scatter(code_[:n0][:,0],code_[:n0][:,1],c=[1,0,0], label = '0')\n",
    "plt.scatter(code_[n0:e1][:,0],code_[n0:e1][:,1],c=[0,1,0], label = '1')\n",
    "plt.scatter(code_[e1:e2][:,0],code_[e1:e2][:,1],c=[0,0,1], label = '2')\n",
    "plt.scatter(code_[e2:][:,0],code_[e2:][:,1],c=[0,0,0], label = '3')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.908890521675\n",
      "0.927820035604\n",
      "0.833820840951\n",
      "0.779025466212\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "print (np.sum((code_[:n0,0] < 0.5)*(code_[:n0,1] < 0.5))+0.0)/n0\n",
    "print (np.sum((code_[n0:e1,0] > 0.5)*(code_[n0:e1,1] > 0.5))+0.0)/n1\n",
    "print (np.sum((code_[e1:e2,0] > 0.5)*(code_[e1:e2,1] < 0.5))+0.0)/n2\n",
    "print (np.sum((code_[e2:,0] < 0.5)*(code_[e2:,1] > 0.5))+0.0)/n3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img1 = train_x[1000].reshape(28,28)\n",
    "plt.imshow(img1,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# img2 = (x__[1100]*sigma + mu).reshape(28,28)\n",
    "img2 = x__[1000].reshape(28,28)\n",
    "plt.imshow(img2,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "code_.shape[0] - n0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
